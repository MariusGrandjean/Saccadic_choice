---
title: "4. Bayesian Analysis"
output: html_document
date: "2024-11-21"
fig_width: 6 
fig_height: 10
---

This is the fourth step of the data analysis pipeline. 

In this code we select models, run diagnostic checking a

1. We begin by loading the required packages .
```{r packages, echo=FALSE, eval=TRUE, message=FALSE, warning=FALSE}
library(tidyverse)
library(tidybayes)
library(readxl)
library(dplyr)
library(patchwork)
library(ggplot2)
library(openxlsx)
library(lme4)
library(lmerTest)
library(stats)
library(emmeans)
library(tidyboot)
library(faintr)
library(brms)
library(stringr)
library(dabestr)
library(ez)
library(MBESS)
```

2. We load the data

```{r loading data, echo=FALSE, eval=TRUE, message=FALSE, warning=FALSE}

base::load("clean_data.rda")

```

3. Writing models

We set the priors and run the models.

3.A. Saccadic Latency

```{r writing models, echo=FALSE, eval=TRUE, message=FALSE, warning=FALSE}
print(mean(clean_data$Latency1))
print(sd(clean_data$Latency1))

# To see what kinds of priors brms is using if we provide none
get_prior(Latency1 ~ 1 + Target * meridian + (1 + Target * meridian|subject),
          data = clean_data)

priors <- c(
  prior("normal(209, 10)", class = "Intercept"),
  prior("normal(0, 15)", class = "b"),
  prior("exponential(0.2)", class = "sd"),
  prior("gamma(2,0.05)", class = "sigma"))

# We run the model
brm1 <- brm(Latency1 ~ 1 + Target * meridian + (1 + Target * meridian|subject),
            data = clean_data,
            family = exgaussian(),
            prior = priors,
            chains = 4, cores = parallel::detectCores(),
            control = list(adapt_delta = 0.95),
            warmup = 3000,iter = 6000)


save(brm1, file="brm1.rda", compress="xz")

tidybayes::summarise_draws(brm1)

plot(brm1)

pp_check(brm1, ndraws = 1e3)

# Optional steps

tidybayes::summarise_draws(brm2)
print("The highest Rhat values -- Should be under 1.01")
brms::rhat(brm1) %>% head()
print("The smallest Effective Sample Size")
brms::neff_ratio(brm1) %>% head()

pars <- variables(brm1)
pars_sel <- c(sample(pars[1:20], 20), sample(pars[-(1:20)], 20))
plot(brm2, pars = pars_sel, N = 9, 
     ask = FALSE, exact_match = TRUE, newpage = TRUE, plot = TRUE)

bayesplot::color_scheme_set("blue")
checkModel <- pp_check(brm2, resp = "Latency1", ndraws = 1e2) +
  # theme(legend.position = "none") +
  xlab("Saccadic reponse time") +
  ylab("Density")
print(checkModel)

# Here we run another model where we add the interaction to check the amount of evidence for the interaction
get_prior(Latency1 ~ 1 + Target * meridian + (1 + Target * meridian|subject),
          data = clean_data)


brm2 <- brm(Latency1 ~ 1 + Target + meridian + (1 + Target + meridian|subject),
            data = clean_data,
            family = exgaussian(),
            prior = priors,
            chains = 4, cores = parallel::detectCores(),
            control = list(adapt_delta = 0.95),
            warmup = 3000,iter = 6000)

save(brm2, file="brm2.rda", compress="xz")

plot(brm2)

pp_check(brm2, ndraws = 1000)

summary(brm2)

# now we run a model only using the main effect of target
brm3 <- brm(Latency1 ~ 1 + Target + (1 + Target|subject),
            data = clean_data,
            family = exgaussian(),
            prior = priors,
            chains = 4, cores = parallel::detectCores(),
            control = list(adapt_delta = 0.95),
            warmup = 3000,iter = 6000)

save(brm3, file="brm3.rda", compress="xz")

plot(brm3)

pp_check(brm3, ndraws = 1000)

# now we run a final model only using the main effect of meridian
brm4 <- brm(Latency1 ~ 1 + meridian + (1 + meridian|subject),
            data = clean_data,
            family = exgaussian(),
            prior = priors,
            chains = 4, cores = parallel::detectCores(),
            control = list(adapt_delta = 0.95),
            warmup = 3000,iter = 6000)

save(brm4, file="brm4.rda", compress="xz")

plot(brm4)

pp_check(brm4, ndraws = 1000)

```

3.B. Saccadic Accuracy

```{r writing models, echo=FALSE, eval=TRUE, message=FALSE, warning=FALSE}
print(mean(clean_data$Accuracy))
print(sd(clean_data$Accuracy))

# Note: The bernoulli family models binary outcomes (e.g., Accuracy = 0 or 1) 
# using a logit link function, which expresses predictors' effects on the log-odds of success.
# This is equivalent to a binomial model with n = 1 trial per observation.

# To see what kinds of priors brms would use by default if none are provided:
get_prior(Accuracy ~ 1 + Target * meridian + (1 + Target * meridian | subject),
          data = clean_data)

# Define priors for the model:
priors <- c(
  # Intercept prior: Assuming baseline accuracy = 75% (log-odds = log(0.75 / 0.25) = 1.1)
  prior("normal(1.1, 2)", class = "Intercept"), 
  # Priors for the fixed effects (b): Centered around 0 with a reasonable variance
  prior("normal(0, 2)", class = "b"),
  # Priors for the standard deviations of random effects: Exponential distribution
  prior("exponential(0.2)", class = "sd"))

# We run the model
brm_acc_1 <- brm(Accuracy ~ 1 + Target * meridian + (1 + Target * meridian|subject),
            data = clean_data,
            family = bernoulli(link = "logit"),
            prior = priors,
            chains = 4, cores = parallel::detectCores(),
            control = list(adapt_delta = 0.95),
            warmup = 3000,iter = 6000)


save(brm_acc_1, file="brm_acc_1.rda", compress="xz")

tidybayes::summarise_draws(brm_acc_1)

plot(brm_acc_1)

pp_check(brm_acc_1, ndraws = 1e3)

# Optional steps

tidybayes::summarise_draws(brm_acc_1)
print("The highest Rhat values -- Should be under 1.01")
brms::rhat(brm_acc_1) %>% head()
print("The smallest Effective Sample Size")
brms::neff_ratio(brm_acc_1) %>% head()

pars <- variables(brm_acc_1)
pars_sel <- c(sample(pars[1:20], 20), sample(pars[-(1:20)], 20))
plot(brm2, pars = pars_sel, N = 9, 
     ask = FALSE, exact_match = TRUE, newpage = TRUE, plot = TRUE)

bayesplot::color_scheme_set("blue")
checkModel <- pp_check(brm_acc_1, resp = "Accuracy", ndraws = 1e2) +
  # theme(legend.position = "none") +
  xlab("Saccadic reponse time") +
  ylab("Density")
print(checkModel)

# Here we run another model where we add the interaction to check the amount of evidence for the interaction
get_prior(Accuracy ~ 1 + Target + meridian + (1 + Target * meridian|subject),
          data = clean_data)


brm_acc_2 <- brm(Accuracy ~ 1 + Target + meridian + (1 + Target + meridian|subject),
            data = clean_data,
            family = bernoulli(link = "logit"),
            prior = priors,
            chains = 4, cores = parallel::detectCores(),
            control = list(adapt_delta = 0.95),
            warmup = 3000,iter = 6000)

save(brm_acc_2, file="brm_acc_2.rda", compress="xz")

plot(brm_acc_2)

pp_check(brm_acc_2, ndraws = 1000)

summary(brm_acc_2)

# now we run a model only using the main effect of target
brm_acc_3 <- brm(Accuracy ~ 1 + Target + (1 + Target|subject),
            data = clean_data,
            family = bernoulli(link = "logit"),
            prior = priors,
            chains = 4, cores = parallel::detectCores(),
            control = list(adapt_delta = 0.95),
            warmup = 3000,iter = 6000)

save(brm_acc_3, file="brm_acc_3.rda", compress="xz")

plot(brm_acc_3)

pp_check(brm_acc_3, ndraws = 1000)

summary(brm_acc_3)

# now we run a final model only using the main effect of meridian
brm_acc_4 <- brm(Accuracy ~ 1 + meridian + (1 + meridian|subject),
            data = clean_data,
            family = bernoulli(link = "logit"),
            prior = priors,
            chains = 4, cores = parallel::detectCores(),
            control = list(adapt_delta = 0.95),
            warmup = 3000,iter = 6000)

save(brm_acc_4, file="brm_acc_4.rda", compress="xz")

plot(brm_acc_4)

pp_check(brm_acc_4, ndraws = 1000)

summary(brm_acc_4)

```

4. Model comparison.

In line with Vehtari et al. (2017), we used the LOO (Leave-One-Out) comparison method to evaluate the models. The goodness of fit for each model was assessed using the Watanabe-Akaike Information Criterion (WAIC), where a lower WAIC value indicates a better model fit (Kurz, 2019; Vehtari et al., 2017). LOO cross-validation was employed to compare the WAIC values and visualize the results. Model 1, with a WAIC of 215505 (Â± 307), emerged as the best-fitting model (see Figure X for the ranking of all models). 

4.A. Saccadic Latency

```{r comparing models, echo=FALSE, eval=TRUE, message=FALSE, warning=FALSE}
# Now we compare the two models
model_fit <- add_criterion(brm1, "waic")
model_fit2 <- add_criterion(brm2,"waic")
model_fit3 <- add_criterion(brm3, "waic")
model_fit4 <- add_criterion(brm4, "waic")

comparaison_model <- as.data.frame(loo_compare(model_fit, model_fit2, model_fit3, model_fit4, criterion = "waic"))
print(comparaison_model, digits = 2, simplify = FALSE)

loo_models <- comparaison_model %>% 
  dplyr::select(waic, se_waic)

# Convert rownames to a column
loo_models$model <- rownames(loo_models) 

loo_models <- loo_models %>%
  filter(model != 'model_fit3') %>%   # Exclude model_fit3
  mutate(model = recode(model,
                        'model_fit' = 'model_1',
                        'model_fit2' = 'model_2',
                        'model_fit4' = 'model_4'))  # Don't need to recode model_fit3


loo_models$model <- factor(loo_models$model, levels = c('model_1', 'model_2', 'model_4'))

# Create the plot for supplementary material
waic_plot <- ggplot(loo_models, aes(x = waic, y = model)) + 
  geom_point(size = 3, color = "black") +  # Plot points for each model
  geom_errorbarh(aes(xmin = waic - se_waic, xmax = waic + se_waic), height = 0.2) +  # Add error bars
  labs(x = "WAIC", y = "Model_name") +  # Axis labels and title
  xlim(215000, 217000) + 
  theme(
    panel.grid.major = element_blank(),  # Remove major grid lines
    panel.grid.minor = element_blank(),  # Remove minor grid lines
    panel.background = element_rect(fill = "white"),  # Set background to white
    axis.line = element_line(colour = "black"),  # Add axis lines
    axis.ticks = element_line(colour = "black")  # Add ticks
  )
# Show the plot
print(waic_plot)

```

4.B. Saccadic Accuracy

```{r comparing models, echo=FALSE, eval=TRUE, message=FALSE, warning=FALSE}
# Now we compare the two models
model_fit <- add_criterion(brm_acc_1, "waic")
model_fit2 <- add_criterion(brm_acc_2,"waic")
model_fit3 <- add_criterion(brm_acc_3, "waic")
model_fit4 <- add_criterion(brm_acc_4, "waic")

comparaison_model <- as.data.frame(loo_compare(model_fit, model_fit2, model_fit3, model_fit4, criterion = "waic"))
print(comparaison_model, digits = 2, simplify = FALSE)

loo_models <- comparaison_model %>% 
  dplyr::select(waic, se_waic)

# Convert rownames to a column
loo_models$model <- rownames(loo_models) 

loo_models <- loo_models %>%
  mutate(model = recode(model,
                        'model_fit' = 'model_1',
                        'model_fit2' = 'model_2',
                        'model_fit3' = 'model_3',
                        'model_fit4' = 'model_4'))  # Don't need to recode model_fit3


loo_models$model <- factor(loo_models$model, levels = c('model_1', 'model_2', 'model_3', 'model_4'))

# Create the plot for supplementary material
waic_plot <- ggplot(loo_models, aes(x = waic, y = model)) + 
  geom_point(size = 3, color = "black") +  # Plot points for each model
  geom_errorbarh(aes(xmin = waic - se_waic, xmax = waic + se_waic), height = 0.2) +  # Add error bars
  labs(x = "WAIC", y = "Model_name") +  # Axis labels and title
  #xlim(215000, 217000) + 
  theme(
    panel.grid.major = element_blank(),  # Remove major grid lines
    panel.grid.minor = element_blank(),  # Remove minor grid lines
    panel.background = element_rect(fill = "white"),  # Set background to white
    axis.line = element_line(colour = "black"),  # Add axis lines
    axis.ticks = element_line(colour = "black")  # Add ticks
  )
# Show the plot
print(waic_plot)

```


5. Analysising the data using model 1.

5.A. Saccadic Latency

```{r comparing models, echo=FALSE, eval=TRUE, message=FALSE, warning=FALSE}

# We begin by looking at the coefficients 

# We begin with retrieving the coefficients from the fixed effects
brm.coefs <- fixef(brm1) %>% as.numeric()

# Due to the organisations of the factors, the faces and the horizontal meridian are considered the baseline
# You can always check this by looking at the name of the variable in the output. 
# For example : b_TargetVehicles means that the baseline is faces and therefore shows the difference between the two.
# In this case it means that the mean reaction time for vehicles is 11.3 ms bigger than for faces
mean_face_hori_coef <- brm.coefs[1] + brm.coefs[2]*0 + brm.coefs[3]*0 # 209

mean_vehicle_hori_coef <- brm.coefs[1] + brm.coefs[2]*1 + brm.coefs[3]*0 # 220: 209 (face when hori/baseline) + 11 (vehicles when hori)

mean_face_vert_coef <- brm.coefs[1] + brm.coefs[2]*0 + brm.coefs[3]*1 # 214 : 209 (face when hori) + 5 (vertical meridian)

mean_vehicle_vert_coef <- brm.coefs[1] + brm.coefs[2]*1 + brm.coefs[3]*1 # 226 : 209 (faces when hori) + 11 (vehicles when hori) + 5 (vertical meridian)


# We look the summary of the model
summary(brm1)

# We obtain the 89% HDI and BF for comparison of groups within the main effects
compare_groups(fit = brm1,
               higher = Target == "Vehicles" ,
               lower = Target =="Faces",
               hdi = 0.89,
               include_bf = TRUE)

compare_groups(fit = brm1,
               higher = meridian == "Vertical" ,
               lower = meridian =="Horizontal",
               hdi = 0.89,
               include_bf = TRUE)

# We obtain the BF for the interaction
brms::hypothesis(brm1, "TargetVehicles:meridianVertical > 0") # for the interaction #fonctionne pas en Rmarkdown

# We obtain the 89% for the interaction (as well as the other main effects just for clearance)
posterior_samples <- posterior_samples(brm1)

main_effects_samples <- posterior_samples[, c("b_TargetVehicles", "b_meridianVertical", "b_TargetVehicles:meridianVertical")]

bayestestR::hdi(main_effects_samples, ci = 0.89) # This is how I obtain HDI for interaction.
# However apparently, this is not taking random effects into account.


```

5.B. Saccadic Accuracy

```{r comparing models, echo=FALSE, eval=TRUE, message=FALSE, warning=FALSE}


# We look the summary of the model
summary(brm_acc_1)

# Our posterior samples are in log-odds, therefore we have to convert them back before comparing the groups

# Extract posterior samples for relevant parameters
posterior <- posterior_samples(
  brm_acc_1, 
  pars = c("b_Intercept", "b_meridianVertical", "b_TargetVehicles")
)

# Log-odds for Meridians
log_odds_horizontal <- posterior$b_Intercept
log_odds_vertical <- posterior$b_Intercept + posterior$b_meridianVertical

# Log-odds for Targets
log_odds_faces <- posterior$b_Intercept
log_odds_vehicles <- posterior$b_Intercept + posterior$b_TargetVehicles

# Convert log-odds to probabilities
prob_horizontal <- plogis(log_odds_horizontal)
prob_vertical <- plogis(log_odds_vertical)
prob_faces <- plogis(log_odds_faces)
prob_vehicles <- plogis(log_odds_vehicles)

# Compute differences in probabilities
prob_diff_meridian <- prob_horizontal - prob_vertical
prob_diff_targets <- prob_faces - prob_vehicles

# Summarize results for Meridians
mean_diff_meridian <- mean(prob_diff_meridian)
hdi_diff_meridian <- bayestestR::hdi(prob_diff_meridian, ci = 0.89)

# Summarize results for Targets
mean_diff_targets <- mean(prob_diff_targets)
hdi_diff_targets <- bayestestR::hdi(prob_diff_targets, ci = 0.89)

# Compute Bayes Factors
# BF_10 = P(H1 | Data) / P(H0 | Data)
bf_meridian <- sum(prob_diff_meridian > 0) / sum(prob_diff_meridian <= 0)
bf_targets <- sum(prob_diff_targets > 0) / sum(prob_diff_targets <= 0)

# Output results
list(
  meridian_difference = list(
    mean_difference = mean_diff_meridian, 
    hdi = hdi_diff_meridian,
    bf_10 = bf_meridian
  ),
  target_difference = list(
    mean_difference = mean_diff_targets, 
    hdi = hdi_diff_targets,
    bf_10 = bf_targets
  )
)

hypothesis(brm_acc_1, "TargetVehicles:meridianVertical > 0")

posterior_samples <- posterior_samples(brm_acc_1)

main_effects_samples <- posterior_samples[, c("b_TargetVehicles", "b_meridianVertical", "b_TargetVehicles:meridianVertical")]

bayestestR::hdi(main_effects_samples, ci = 0.89)

```

--> This conclude the analysis pipeline for this task.

